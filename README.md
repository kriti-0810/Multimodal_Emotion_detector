# Multimodal Emotion Detector

An AI-powered application that detects human emotions through:
- ğŸ“ Text (typed or uploaded)
- ğŸ”Š Speech (microphone or audio file)
- ğŸ“· Facial expression (image or webcam)

This project uses machine learning and deep learning techniques to classify emotions like happy, sad, angry, fear, neutral, and more.

More details coming soon!
# ğŸ§  Multimodal Emotion Detector

An intelligent system that detects emotions from **text**, **speech**, and **facial expressions**, then combines the results into a unified emotion output with confidence and emojis! ğŸ­ğŸ”ŠğŸ“·âœï¸

---

## ğŸš€ Features

- ğŸ”¤ Text Emotion Detection (BERT/LSTM)
- ğŸ”Š Speech Emotion Detection (RAVDESS Dataset, MFCC + RandomForest)
- ğŸ˜„ Facial Emotion Recognition (FER-2013 + CNN)
- ğŸ“¸ Webcam & file-based detection
- ğŸ™ Live mic recording
- ğŸ“‚ Upload image/audio/text files
- âœ… Unified interface using Streamlit
- ğŸ§  Displays detected emotion + emoji + confidence

---

## ğŸ“ Project Structure

```
Multimodal_Emotion_Detector/
â”œâ”€â”€ app.py                  # Streamlit frontend
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ models/                 # Trained model files (.pkl, .h5)
â”œâ”€â”€ utils/                  # Text/speech/facial modules
â”‚   â”œâ”€â”€ text_emotion.py
â”‚   â”œâ”€â”€ speech_emotion.py
â”‚   â”œâ”€â”€ facial_emotion.py
â”œâ”€â”€ datasets/               # Datasets (you must download manually)
â”œâ”€â”€ static/                 # Icons, emojis, images
â”œâ”€â”€ test_images/            # Sample image inputs
â””â”€â”€ README.md               # Project instructions
```

---

## ğŸ”§ Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/kriti-0810/Multimodal_Emotion_detector.git
   cd Multimodal_Emotion_detector
   ```

2. **Set up virtual environment**
   ```bash
   python -m venv emotion_env
   emotion_env\Scripts\activate   # On Windows
   ```

3. **Install requirements**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“¥ Download Datasets (Required!)

This project requires two datasets. **They are not included in the repo**. Download them manually:

| Dataset | Purpose | Download |
|--------|--------|----------|
| FER-2013 | Facial emotion detection | https://www.kaggle.com/datasets/msambare/fer2013 |
| RAVDESS | Speech emotion detection | https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio |

After download, place them like this:
```
datasets/
â”œâ”€â”€ FER-2013/
â”œâ”€â”€ RAVDESS/
```

---

## ğŸƒâ€â™€ï¸ Running the App

```bash
streamlit run app.py
```

Visit [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ’¡ Demo Inputs (Optional)

Put test files in `test_images/` or use the upload buttons inside the app.

---

## ğŸ“Œ Notes

- The project is modular; each component can be run/tested separately.
- Ensure your system has `ffmpeg`, `OpenCV`, and compatible Python version (3.7+).
- If model files are missing, you may need to retrain using the scripts in `utils/`.

---

## ğŸ‘©â€ğŸ’» Author

> ğŸ”— [Kriti](https://github.com/kriti-0810)  
> ğŸ“ BTech CSE | Passionate about AI/ML | VIT Vellore  
> ğŸ¤ Open to collaboration and learning!

---

## ğŸ“œ License

This project is for educational purposes.
